# RAG Powered Knowledge Tracker 

## Abstract
This is a personal DB with RAG capabilities to keep track of the research papers, slides, documents I have been reading (and the notes I have been taking) on different topics. 
The purpose is to test/evaluate embedding techniques and vector search and vector databases, to organise and maintain the things I have been learning (and eventually to implement a few-shot, LLM-based, classification of items in categories).

Constraints by desing: 
- Lightweight and low-latency;
- Topics must be divided into main categories, e.g., {Quantum Technologies, AI, Space Telecom, ...};
These categories are then split into sub-categories, e.g., {Quantum Technologies/Quantum Computing}, or {Quantum Technologies/QKD}, etc.;
- Assign a score of relevance (e.g., from 0 to 5 stars) either manually (me) or automatically (e.g., based on the number of times I open a document);
- The final outcome is displayed through a simple GUI. 

The input in my case were messy documents: research papers, personal notes, etc. The format of those might be PDF, but is not strictly PDF (e.g., may be .docs).


### What embedding model am I using? 
For a lightweight and low-latency solution on small data base like mine, I use the all-MiniLM-L6-v2 model. To clarify, MiniLM-L6-v2 is a single neural network architecture that acts as both the Document Encoder and the Query Encoder. all-MiniLM-L6-v2 is a Sentence Transformer (with bi-encoder achitecture) that is pre-trained to map sentences (both query and document chunk) into a dense vector space where semantic meaning is captured by proximity. 
- The Document Encoder is the SentenceTransformer model (loaded in ingest.py).	This neural network encodes every document chunk (D) into a vector $V_D$. This happens once during ingestion.
- The Query Encoder	is still the SentenceTransformer model (loaded in retrieve_rag.py	this time). This same network encodes the user query (Q) into a vector  $V_Q$. This happens every time a query is made.
Note that the most commong RAG architectures uses two neural nets (bi-encoder) because Q&A are semantically different. We can think of the whole model as made up of two parts: the query part and the document part (see diagram below).

The document encoder and the query encoder share the same weights (they are the same model instance, all-MiniLM-L6-v2). This allows you to pre-compute the document vectors and store them in FAISS, making retrieval extremely fast. This is the most common and efficient architecture for the retrieval step in modern RAG systems.
To avoid confusion (one might think that the neural nets for Q&A are trained on different data sets), models like all-MiniLM-L6-v2 are fine-tuned on vast datasets of (Query, Relevant Document, Non-Relevant Document) triplets using a technique called Contrastive Learning. This explicitly trains the single model to ensure that a relevant document's vector is closer to the query vector than a non-relevant document's vector.

<p align="center">
<img width="673" height="486" alt="Screenshot 2025-11-18 at 11 03 19" src="https://github.com/user-attachments/assets/790f57cc-59d4-4699-8967-50cc0c126b63" />

### Retrieval Mechanism
Finally, the retrieval mechanismis is provided by FAISS, an open-source library for similarity search and clustering of vectors. Retrieval finds the nearest document vectors ($V_D$) to the query vector ($V_Q$), using distance (L2 or Cosine).

### System's Vector Database
I implement a basic, custom Vector Store by combining two parts:

- FAISS Index: Stores the high-dimensional vectors (faiss_index.bin).
- SQLite Database: Stores the metadata (the document text chunks, document titles, chunk IDs, etc.) that corresponds to the vectors in the FAISS index.


### Comments on the scripts 
- **ingest.py** (ingestion) code scans a folder of documents, extracts text (PDF/DOCX/TXT/MD), chunks text (configurable chunk size + overlap), computes embeddings (sentence-transformers), and stores metadata in SQLite and vectors in FAISS. 
Note that if one prefers an external embedding API (OpenAI, Google, ...), replace the model.encode call with embedding API (but keep vector normalization for FAISS). For very large libraries, consider using IndexIVFFlat with training for FAISS (but that complicates persistence).

-  **retrieve_rag.py** (RAG) simply takes top N chunks, concatenate them with short separators and citations (title - page/ chunk idx). Add system + user instructions, then call LLM (locally or via API). 
Example prompt is:
~~~
System: You are a helpful research assistant. Use only the provided document excerpts to answer the question.
If insufficient, say "Insufficient evidence".

Context:
[Document A] (title) — excerpt...
[Document B] — excerpt...

Question: <user question>
~~~
### How does the GUI look? 

<p align="center">
<img width="600" height="800" alt="Screenshot 2025-11-18 at 08 40 32" src="https://github.com/user-attachments/assets/93bcbef8-4714-4ec3-a940-969e23f138ab" />

### Notes & enhancements

Note: Chunk size: 150–500 tokens is typical / Overlap helps continuity / Adjust for your file types (slides short; papers longer) / Keep embeddings dimension modest (e.g., 384) to keep memory low.

- Scalability: FAISS (IndexFlatIP) works great (low latency) for small-to-medium corpora (thousands). For many thousands or millions, move to an IVF index with training or use a small vector DB (Chroma/Weaviate/Milvus;  but at the cost of more complexity).
- Embedding quality evaluation: try to build a "golden dataset" and test with common metrics such as precision, recall, nDCG.
- Embedding model choices. all-MiniLM-L6-v2 (sentence-transformers) gives compact 384-d embeddings, fast, no external API. For higher-quality embeddings, one can try all-mpnet-base-v2 (768-d) or an API (e.g., Google embedding models).
- Security & Privacy: keep embeddings and local files private (be aware when using cloud API). 
- The ingestion script currently appends new chunks and adds to FAISS. If you change chunking/ embedding model, rebuild index from scratch (re-ingest).
- Re-ranking (very optional): re-rank FAISS results with cross-encoder model for higher accuracy (compute cost higher; can be selective on top 20 results before final top-K).
- Few-shot, LLM-based, classification of new items into the right category. 
.......................................................................

When you first run the code: 
~~~
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
~~~

### References
Google Whitepaper, "_Embeddings & Vector Stores_", Anant Nawalgaria, Xiaoqi Ren, and Charles Sugnet.
